# Access PyTorch C/C++ Implementation from Python

## Codebase structure

Only list a simplified structure that PyTorch C++ implementation and Python binding use. Tooling, testing, and legacy directories are not listed here.

- c10 - Core library files that work everywhere and only contain essential functionality. Files in Aten/Core are slowly being moved here.
- aten - C++ tensor library for PyTorch (no autograd support)
    - [src/ATen](https://github.com/pytorch/pytorch/tree/master/aten/src/ATen)
        - [core](https://github.com/pytorch/pytorch/tree/master/aten/src/ATen/core) - Core functionality of ATen. This
        is migrating to top-level c10 folder.
        - [native](https://github.com/pytorch/pytorch/tree/master/aten/src/ATen/native) - Modern implementations of
        operators. If you want to write a new operator, here is where
        it should go. Most CPU operators go in the top level directory,
        except for operators which need to be compiled specially; see
        cpu below.
            - [cpu](https://github.com/pytorch/pytorch/tree/master/aten/src/ATen/native/cpu) - Not actually CPU
            implementations of operators, but specifically implementations
            which are compiled with processor-specific instructions, like
            AVX. See the [README](https://github.com/pytorch/pytorch/tree/master/aten/src/ATen/native/cpu/README.md) for more
            details.
            - [cuda](https://github.com/pytorch/pytorch/tree/master/aten/src/ATen/native/cuda) - CUDA implementations of
            operators.
            - [sparse](https://github.com/pytorch/pytorch/tree/master/aten/src/ATen/native/sparse) - CPU and CUDA
            implementations of COO sparse tensor operations
            - [mkl](https://github.com/pytorch/pytorch/tree/master/aten/src/ATen/native/mkl) [mkldnn](https://github.com/pytorch/pytorch/tree/master/aten/src/ATen/native/mkldnn)
            [miopen](https://github.com/pytorch/pytorch/tree/master/aten/src/ATen/native/miopen) [cudnn](https://github.com/pytorch/pytorch/tree/master/aten/src/ATen/native/cudnn)
            - implementations of operators which simply bind to some
            backend library.
            - [quantized](https://github.com/pytorch/pytorch/tree/master/aten/src/ATen/native/quantized/) - Quantized tensor (i.e. QTensor) operation implementations.
- torch
    - [_C](https://github.com/pytorch/pytorch/tree/master/torch/_C/) - Python stubs/interface files that provide auto-complete functionality, some of lines in these files are auto generated by [gen_pyi.py](https://github.com/pytorch/pytorch/tree/master/tools/pyi/gen_pyi.py). The underlying C interfaces bridging C++ implementation are compiled as Python extensions when installing PyTorch from source.
    - [csrc](https://github.com/pytorch/pytorch/tree/master/torch/_C/) - C++ files composing the PyTorch library. Files
    in this directory tree are a mix of Python binding code, and C++
    heavy lifting. Consult `setup.py` for the canonical list of Python
    binding files; conventionally, they are often prefixed with
    `python_`. [README](https://github.com/pytorch/pytorch/tree/master/torch/csrc/README.md)
        - [autograd](https://github.com/pytorch/pytorch/tree/master/torch/csrc/autograd) - Implementation of reverse-mode automatic differentiation. [README](https://github.com/pytorch/pytorch/tree/master/torch/csrc/autograd/README.md)
    - Python bindings - Python codes that access C++ interface and implementation through the extension module in `torch._C` namespace.

Reference:

- [CONTRIBUTING.md/Codebase structure](https://github.com/pytorch/pytorch/blob/master/CONTRIBUTING.md#codebase-structure)

## Calling extenstion modules from the Python binding

Let's take a look at a PyTorch module source code.

`torch.nn.GeLU`
```python
    def forward(self, input: Tensor) -> Tensor:
        # Here it calls `F.gelu`, which stands for `torch.nn.functional.gelu`
        return F.gelu(input)
```

`torch.nn.functional.gelu`
```Python
    def gelu(input):
        r"""gelu(input) -> Tensor

        Applies element-wise the function
        :math:`\text{GELU}(x) = x * \Phi(x)`

        where :math:`\Phi(x)` is the Cumulative Distribution Function for Gaussian Distribution.

        See `Gaussian Error Linear Units (GELUs) <https://arxiv.org/abs/1606.08415>`_.
        """
        if has_torch_function_unary(input):
            return handle_torch_function(gelu, (input,), input)
        
        # Here it calls `torch._C._nn.gelu`, which is a function in `torch._C` extension module.
        return torch._C._nn.gelu(input)
```

## Building the C/C++ interface and implementation

PyTorch uses Python built-in extention module to compile C/C++ files.

`torch._C` extension module is defined in `setup.py->configure_extension_build`. The included files are defined in `MANIFEST.in`
```Python
    C = Extension("torch._C",
            libraries=main_libraries, # ['torch_python']
            sources=main_sources, # ["torch/csrc/stub.c"] # entry point
            language='c',
            extra_compile_args=main_compile_args + extra_compile_args,
            include_dirs=[],
            library_dirs=library_dirs,
            extra_link_args=extra_link_args + main_link_args + make_relative_rpath_args('lib')
    )
    extensions.append(C)
```

The entry point:

`torch/csrc/stub.c`
```c
PyMODINIT_FUNC PyInit__C(void)
{
  return initModule(); // calls initModule() in `torch/csrc/Module.cpp`
}
```

## Native functions & dispatch

All native functions are listed in [nation_functions.yaml](https://github.com/pytorch/pytorch/tree/master/aten/src/ATen/native/native_functions.yaml), which defines native functions' signature, properties, and dispatches. The ATen C/C++ interfaces are generated from this file for the Python binding and LibTorch.

Let's take a look at the definitions of GeLU activation:

`aten/src/ATen/native/native_functions.yaml`
```yaml
# Forwords
- func: gelu.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
  structured: True
  structured_inherits: TensorIteratorBase
  device_check: NoCheck   # TensorIterator
  # Which python extension module this function belongs to
  python_module: nn
  # Define dispatched functions according to the backend
  dispatch:
    # For cpu backend, the computation is dispatched to `gelu_out_cpu` function located in `aten/src/ATen/native/Activation.cpp`
    CPU: gelu_out_cpu
    # For cuda backend, the computation is dispatched to `gelu_out_cuda` function located in `aten/src/ATen/native/cuda/Activation.cpp`
    CUDA: gelu_out_cuda

- func: gelu(Tensor self) -> Tensor
  structured_delegate: gelu.out
  device_check: NoCheck   # TensorIterator
  python_module: nn
  dispatch:
    MkldnnCPU: mkldnn_gelu
    QuantizedCPU: gelu_quantized_cpu

# Backwards
- func: gelu_backward.grad_input(Tensor grad, Tensor self, *, Tensor(a!) grad_input) -> Tensor(a!)
  structured: True
  structured_inherits: TensorIteratorBase
  python_module: nn
  dispatch:
    CPU: gelu_backward_out_cpu
    CUDA: gelu_backward_out_cuda

- func: gelu_backward(Tensor grad, Tensor self) -> Tensor
  structured_delegate: gelu_backward.grad_input
  python_module: nn
  dispatch:
    MkldnnCPU: mkldnn_gelu_backward
```

### CPU Dispatch

`aten/src/ATen/native/Activation.cpp`
```cpp
TORCH_IMPL_FUNC(gelu_out_cpu) (
  const Tensor& self, const Tensor& result
) {
#if AT_MKLDNN_ENABLED()
  if (use_mkldnn(self)) {
    const ideep::tensor& x = itensor_from_tensor(self);
    ideep::tensor y = itensor_from_tensor(result);
    ideep::eltwise_forward::compute(
      x, y, ideep::algorithm::eltwise_gelu_erf, ideep::prop_kind::forward_training, /*alpha*/ 0.0);
  } else {
    GeluKernel(kCPU, *this);
  }
#else
  // If no mkldnn enabled, the computation is dispatched to registered `GeluKernelImpl` function located in `aten/src/ATen/native/cpu/Activation.cpp`
  GeluKernel(kCPU, *this);
#endif
}
```

The actual C++ implementation of GeLU for cpu backend:

`aten/src/ATen/native/cpu/Activation.cpp`
```cpp
void GeluKernelImpl(TensorIteratorBase& it) {
  auto grain_size = at::internal::GRAIN_SIZE;
  // Numbers based on benchmarking.
  // Benchmark: benchmarks/operator_benchmarks/pt/gelu_test.py
#ifdef C10_MOBILE
  // Benchmarked on S8 US phone.
  // Internal benchmarking that converts operator benchmark into
  // a torchscript module and run that on mobile.
  // Same benchmark as server side.
  constexpr int64_t GELU_MIN_ELEMENTS_FOR_MULTI_THREADING{6144};
#else
  // Benchmarked on i9 8 core 16 thread machine.
  // 1 thread: cd benchmark/operator_benchmarks;
  //           python -m pt.gelu_test --tag_filter long --omp_num_threads 1
  // 2 threads: cd benchmark/operator_benchmarks;
  //           python -m pt.gelu_test --tag_filter long --omp_num_threads 1
  constexpr int64_t GELU_MIN_ELEMENTS_FOR_MULTI_THREADING{16384};
#endif
  if (it.numel() > GELU_MIN_ELEMENTS_FOR_MULTI_THREADING) {
    grain_size = it.numel() / at::get_num_threads();
  }
  AT_DISPATCH_FLOATING_TYPES_AND(
      ScalarType::BFloat16, it.dtype(), "GeluKernelImpl", [&]() {
    using Vec = vec::Vectorized<scalar_t>;
    const Vec kAlphaVec(scalar_t(M_SQRT1_2));
    const Vec kOneVec(scalar_t(1));
    const Vec kPointFiveVec(scalar_t(0.5));
    cpu_kernel_vec(
        it,
        [](scalar_t x) {
          const scalar_t kAlpha = scalar_t(M_SQRT1_2);
          return x * scalar_t(0.5) * (scalar_t(1) + std::erf(x * kAlpha));
        },
        [&](Vec x_vec) {
          return x_vec * kPointFiveVec *
              (kOneVec + (x_vec * kAlphaVec).erf());
        },
        grain_size);
  });
}
```

### GPU Dispatch

`aten/src/ATen/native/cuda/Activation.cpp`
```cpp
TORCH_IMPL_FUNC(gelu_out_cuda) (
    const Tensor& /*self*/, const Tensor& /*result*/
  ) {
  GeluCUDAKernelImpl(*this);
}
```

The actual C++ implementation of GeLU for gpu backend:

`aten/src/ATen/native/cuda/Activation.cu`

```cpp
void GeluCUDAKernelImpl(TensorIteratorBase& it) {
  AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, it.dtype(), "GeluCUDAKernelImpl", [&]() {
    using T_ACC = acc_type<scalar_t, true>;
    gpu_kernel(it, [] GPU_LAMBDA(scalar_t x) -> scalar_t {
      return static_cast<T_ACC>(x) *
          c10::cuda::compat::normcdf(static_cast<T_ACC>(x));
    });
  });
}
```

### Generated ATen function

`aten/src/ATen/Functions.cpp`, which is generated by `tools/codegen/gen.py`
```cpp
// aten::gelu(Tensor self) -> Tensor
at::Tensor gelu(const at::Tensor & self) {
    static auto op = c10::Dispatcher::singleton()
        .findSchemaOrThrow("aten::gelu", "")
        .typed<at::Tensor (const at::Tensor &)>();
    return op.call(self);
}

// aten::gelu_backward(Tensor grad, Tensor self) -> Tensor
at::Tensor gelu_backward(const at::Tensor & grad, const at::Tensor & self) {
    static auto op = c10::Dispatcher::singleton()
        .findSchemaOrThrow("aten::gelu_backward", "")
        .typed<at::Tensor (const at::Tensor &, const at::Tensor &)>();
    return op.call(grad, self);
}
```

## Autograd

Autograd provides automatic differentiation funcationalities for PyTorch.

All derivative definitions are listed in [derivatives.yaml](https://github.com/pytorch/pytorch/tree/master/tools/autograd/derivatives.yaml), which indicates forward functions and the corresponding backward functions defined in `native_functions.yaml`.

Let's take GeLU activation again for example:

`tools/autograd/derivatives.yaml`

```yaml
- name: gelu(Tensor self) -> Tensor # The forward function defined in `native_functions.yaml`.
  self: "GradMode::is_enabled() ? infinitely_differentiable_gelu_backward(grad, self) : gelu_backward(grad, self)" # The backward function defined in `native_functions.yaml`.
  result: auto_element_wise
```

The C/C++ interfaces for the Python binding are generated using scripts in  `tools/autograd/gen_xxx.py` using templates located in `tools/autograd/templates/`

GeLU belongs to `torch.nn` namespace. The generated GeLU C/C++ interface looks like this:

`torch/csrc/autograd/generated/python_nn_functions.cpp`

`initNNFunctions()` is the initialization function, which is called from `torch/csrc/Module.cpp`:

<details>
  <summary>torch._c namespaces and corresponding files (not completed yet)</summary>
  
  Required to be initialized:
  
  - `torch._C.*` -> `torch/csrc/autograd/python_variable.cpp` with init function `THPVariable_initModule`
      - `torch._C._TensorBase`
      - `torch._C._TensorMeta`
      - `torch._C.*` -> `torch/csrc/autograd/generated/python_torch_functions.cpp` with init function `initTorchFunctions`
  - `torch._C._FunctionBase.*` ->  `torch/csrc/autograd/python_function.cpp` with init function `THPFunction_initModule`
  - `torch._C._ImperativeEngine.*` -> `torch/csrc/autograd/python_engine.cpp` with init function `THPEngine_initModule`

  ---

  Optional to be initialized:

</details>

```cpp
static PyMethodDef nn_functions[] = {
    // omit above...
    {"gelu", castPyCFunctionWithKeywords(THPVariable_gelu), METH_VARARGS | METH_KEYWORDS, NULL},
    // omit below...
}

void initNNFunctions(PyObject* module) {
  static struct PyModuleDef def = {
     PyModuleDef_HEAD_INIT,
     "torch._C._nn",
     NULL,
     -1,
     nn_functions
  };
  PyObject* nn = PyModule_Create(&def);
  THPNNVariableFunctionsModule = nn;
  if (!nn) {
    throw python_error();
  }
  // steals a reference to nn
  if (PyModule_AddObject(module, "_nn", nn) != 0) {
    throw python_error();
  }
}
```

Forward interface:

```cpp
static PyObject * THPVariable_gelu(PyObject* self_, PyObject* args, PyObject* kwargs)
{
  HANDLE_TH_ERRORS
  static PythonArgParser parser({
    "gelu(Tensor input, *, Tensor out=None)",
  }, /*traceable=*/true);

  ParsedArgs<2> parsed_args;
  auto _r = parser.parse(nullptr, args, kwargs, parsed_args);
  if(_r.has_torch_function()) {
    return handle_torch_function(_r, nullptr, args, kwargs, THPNNVariableFunctionsModule, "torch.nn");
  }
  if (_r.isNone(1)) {
    // aten::gelu(Tensor self) -> Tensor
    
    auto dispatch_gelu = [](const at::Tensor & self) -> at::Tensor {
      pybind11::gil_scoped_release no_gil;
      return at::gelu(self);
    };
    return wrap(dispatch_gelu(_r.tensor(0)));
  } else {
    // aten::gelu.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
    
    auto dispatch_gelu_out = [](at::Tensor out, const at::Tensor & self) -> at::Tensor {
      pybind11::gil_scoped_release no_gil;
      return at::gelu_out(out, self);
    };
    return wrap(dispatch_gelu_out(_r.tensor(1), _r.tensor(0)));
  }
  Py_RETURN_NONE;
  END_HANDLE_TH_ERRORS
}
```

Backward interface:

`torch/csrc/autograd/generated/Functions.cpp`
```cpp
variable_list GeluBackward::apply(variable_list&& grads) {
  std::lock_guard<std::mutex> lock(mutex_);

  IndexRangeGenerator gen;
  auto self_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  auto self = self_.unpack();
  bool any_grad_defined = any_variable_defined(grads);
  if (should_compute_output({ self_ix })) {
    auto grad_result = any_grad_defined ? (GradMode::is_enabled() ? infinitely_differentiable_gelu_backward(grad, self) : gelu_backward(grad, self)) : Tensor();
    copy_range(grad_inputs, self_ix, grad_result);
  }
  return grad_inputs;
}
```